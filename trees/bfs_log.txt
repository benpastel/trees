TREE_COUNT=100


Agaricus:

/Users/benji/dev/trees/trees/benchmarks.py:43: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  return X_svm.toarray(), y.astype(np.bool)
X.shape: train (5211, 126), valid (1302, 126)
binary classification with 2518 true and 2693 false
train xgboost with: {'n_estimators': 100, 'tree_method': 'hist'}...
/usr/local/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
  warnings.warn(label_encoder_deprecation_msg, UserWarning)
[04:06:50] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
(0.1s)
predict xgboost...
(0.0s)

      train preds bincount: [2693 2518]
      train: accuracy = 100.00%, precision = 100.00%, recall = 100.00%
      valid: accuracy = 100.00%, precision = 100.00%, recall = 100.00%


Params(smooth_factor=100.0, weight_smooth_factor=100.0, max_nodes=3280, max_depth=8, tree_count=100, learning_rate=0.3, third_split_penalty=4.0, bucket_count=64, bucket_sample_count=10000, trees_per_bucketing=2): train our tree ...
(7.9s)
Classification model with 100 trees with sizes min=4 max=34 mean=10.66
  predict our tree...
(0.0s)

      train preds bincount: [2695 2516]
      train: accuracy = 99.96%, precision = 100.00%, recall = 99.92%
      valid: accuracy = 99.85%, precision = 100.00%, recall = 99.68%



House Prices:

X.shape: train (1168, 79), valid (292, 79)
regression targets with min=35311.0, max=755000.0, mean=181216.2
train xgboost with: {'n_estimators': 100, 'tree_method': 'hist'}...
(0.2s)
predict xgboost...
(0.0s)

      train: log(MSE): 13.97, MAE: 752.35
      valid: log(MSE): 20.21, MAE: 16691.15


Params(smooth_factor=100.0, weight_smooth_factor=100.0, max_nodes=3280, max_depth=8, tree_count=100, learning_rate=0.3, third_split_penalty=4.0, bucket_count=64, bucket_sample_count=10000, trees_per_bucketing=2): train our tree ...
(1.2s)
Regression model with 21 trees with sizes min=1 max=10 mean=6.0
  predict our tree...
(0.0s)

      train: log(MSE): 21.07, MAE: 24163.66
      valid: log(MSE): 20.84, MAE: 22846.03



Home Credit Default:

/Users/benji/dev/trees/trees/benchmarks.py:76: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  y = frame['TARGET'].values.astype(np.bool)
X.shape: train (246009, 104), valid (61502, 104)
binary classification with 19842 true and 226167 false
train xgboost with: {'n_estimators': 100, 'tree_method': 'hist'}...
[04:07:02] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
(1.8s)
predict xgboost...
(0.2s)

      train preds bincount: [244702   1307]
      train: accuracy = 92.26%, precision = 80.64%, recall = 5.31%
      valid: accuracy = 91.90%, precision = 49.80%, recall = 2.53%


Params(smooth_factor=100.0, weight_smooth_factor=100.0, max_nodes=3280, max_depth=8, tree_count=100, learning_rate=0.3, third_split_penalty=4.0, bucket_count=64, bucket_sample_count=10000, trees_per_bucketing=2): train our tree ...
(1.8s)
Classification model with 13 trees with sizes min=1 max=52 mean=22.923076923076923
  predict our tree...
(0.1s)

      train preds bincount: [246009]
      train: accuracy = 91.93%, precision = 0.00%, recall = 0.00%
      valid: accuracy = 91.90%, precision = 0.00%, recall = 0.00%



Santander Value:

X.shape: train (3568, 4991), valid (891, 4991)
regression targets with min=30000.0, max=40000000.0, mean=5951684.1
train xgboost with: {'n_estimators': 100, 'tree_method': 'hist'}...
(9.0s)
predict xgboost...
(0.1s)

      train: log(MSE): 29.97, MAE: 2214584.93
      valid: log(MSE): 31.75, MAE: 5184175.40


Params(smooth_factor=100.0, weight_smooth_factor=100.0, max_nodes=3280, max_depth=8, tree_count=100, learning_rate=0.3, third_split_penalty=4.0, bucket_count=64, bucket_sample_count=10000, trees_per_bucketing=2): train our tree ...
(12.7s)
Regression model with 10 trees with sizes min=1 max=7 mean=4.3
  predict our tree...
(0.0s)

      train: log(MSE): 31.67, MAE: 5424906.88
      valid: log(MSE): 31.70, MAE: 5482966.45



M5:

loading from cache
X.shape: train (37734424, 19), valid (9433606, 19)
regression targets with min=0.0, max=763.0, mean=1.2
train xgboost with: {'n_estimators': 100, 'tree_method': 'hist'}...
(78.6s)
predict xgboost...
(17.0s)

      train: MSE: 4.44, MAE: 0.83
      valid: MSE: 4.46, MAE: 0.83


Params(smooth_factor=100.0, weight_smooth_factor=100.0, max_nodes=3280, max_depth=8, tree_count=100, learning_rate=0.3, third_split_penalty=4.0, bucket_count=64, bucket_sample_count=10000, trees_per_bucketing=2): train our tree ...
(127.8s)
Regression model with 100 trees with sizes min=598 max=3280 mean=1741.09
  predict our tree...
(25.0s)

      train: MSE: 4.25, MAE: 0.83
      valid: MSE: 4.35, MAE: 0.83



Grupo:

loading from cache
X.shape: train (33112214, 75), valid (8278053, 75)
regression targets with min=0.0, max=5000.0, mean=7.3
train xgboost with: {'n_estimators': 100, 'tree_method': 'hist'}...
(128.5s)
predict xgboost...
(24.6s)

      train: MSE: 134.83, MAE: 3.67
      valid: MSE: 154.33, MAE: 3.69


Params(smooth_factor=100.0, weight_smooth_factor=100.0, max_nodes=3280, max_depth=8, tree_count=100, learning_rate=0.3, third_split_penalty=4.0, bucket_count=64, bucket_sample_count=10000, trees_per_bucketing=2): train our tree ...
(328.2s)
Regression model with 100 trees with sizes min=1000 max=3280 mean=1915.27
  predict our tree...
(64.0s)

      train: MSE: 144.84, MAE: 3.58
      valid: MSE: 155.20, MAE: 3.60